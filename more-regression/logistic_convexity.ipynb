{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a7b96f8",
   "metadata": {},
   "source": [
    "# What's wrong with sum of squares for logistic regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ef543f",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Safe settings for Pandas.\n",
    "pd.set_option('mode.chained_assignment', 'raise')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# Make the plots look more fancy.\n",
    "plt.style.use('fivethirtyeight')\n",
    "# Optimization function\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6066dbc3",
   "metadata": {},
   "source": [
    "This page gives some extra explanation for the\n",
    "[logistic_regression](logistic_regression) page.\n",
    "\n",
    "Here we say more about why we might prefer the Maximum Likelihood Estimate way\n",
    "of scoring potential fits to the data, to our more usual least squared error.\n",
    "If you want the gory details on this choice, see [this answer on\n",
    "StackOverflow](https://stats.stackexchange.com/a/254067). Here we look at\n",
    "whether this the sum of squares works well with `minimize`.  The discussion in\n",
    "this page corresponds to the \"computational efficiency\" section of the answer\n",
    "linked above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c52618d",
   "metadata": {},
   "source": [
    "## The background, the data\n",
    "\n",
    "In that page we were trying to looking at the [chronic kidney disease\n",
    "dataset](../data/chronic_kidney_disease), to see if we good predict whether a\n",
    "patient had \"good\" appetite (as opposed to \"poor\" appetite) given that patient's blood hemoglobin concentration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad28fd4b",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('ckd_clean.csv')\n",
    "# Our columns of interest.\n",
    "hgb_app = df.loc[:, ['Hemoglobin', 'Appetite']].copy()\n",
    "# Dummy value column containing 0 for \"poor\" Appetite, 1 for \"good\".\n",
    "hgb_app['appetite_dummy'] = hgb_app['Appetite'].replace(\n",
    "    ['poor', 'good'],\n",
    "    [0, 1])\n",
    "hgb_app.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ed142e",
   "metadata": {},
   "source": [
    "We take out the columns we are interested in for our further use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64af6009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The x (predictor) and y (to-be-predicted) variables.\n",
    "hemoglobin = hgb_app['Hemoglobin']\n",
    "appetite_d = hgb_app['appetite_dummy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f0efd7",
   "metadata": {},
   "source": [
    "Here is a plot of the 0 / 1 appetite values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5030d8bc",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_hgb_app():\n",
    "    # Build plot, add custom label.\n",
    "    colors = hgb_app['Appetite'].replace(['poor', 'good'], ['red', 'blue'])\n",
    "    hgb_app.plot.scatter('Hemoglobin', 'appetite_dummy', c=colors)\n",
    "    plt.ylabel('Appetite\\n0 = poor, 1 = good')\n",
    "    plt.yticks([0,1]);  # Just label 0 and 1 on the y axis.\n",
    "    # Put a custom legend on the plot.  This code is a little obscure.\n",
    "    plt.scatter([], [], c='blue', label='good')\n",
    "    plt.scatter([], [], c='red', label='poor')\n",
    "\n",
    "plot_hgb_app()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c728493f",
   "metadata": {},
   "source": [
    "## Linear regression - the crude approach\n",
    "\n",
    "The crude and brutal approach to predicting these values is to use simple\n",
    "least-squares regression.   We can do this in the usual way by using\n",
    "`scipy.optimize.minimize` with a function that returns the sum of squared\n",
    "error between the straight line predictions and the 0 / 1 labels.  Here's the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ab0eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ss_cost(c_s, x_values, y_values):\n",
    "    \"\"\" Cost function for sum of squares prediction error\n",
    "    \"\"\"\n",
    "    # c_s is a list containing two elements, an intercept and a slope.\n",
    "    intercept, slope = c_s\n",
    "    # Values predicted from these x_values, using this intercept and slope.\n",
    "    predicted = intercept + x_values * slope\n",
    "    # Difference of prediction from the actual y values.\n",
    "    error = y_values - predicted\n",
    "    # Sum of squared error.\n",
    "    return np.sum(error ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acff272e",
   "metadata": {},
   "source": [
    "Start with a guess of intercept -0.5, slope 0.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cb2c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cost function with miminize.\n",
    "mr_ss = minimize(ss_cost, [-0.5, 0.1], args=(hemoglobin, appetite_d))\n",
    "mr_ss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e0394f",
   "metadata": {},
   "source": [
    "Store the slope and intercept, predict the values directly from the straight line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e7775",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_ss, slope_ss = mr_ss.x\n",
    "predicted_ss = inter_ss + slope_ss * hemoglobin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf791a7",
   "metadata": {},
   "source": [
    "Show the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cb8249",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Do the base plot of the hemoglobin and appetite_d.\n",
    "plot_hgb_app()\n",
    "\n",
    "# A new plot on top of the old.\n",
    "plt.scatter(hemoglobin, predicted_ss,\n",
    "            label='LR prediction',\n",
    "            color='orange')\n",
    "plt.title(\"Linear regression with sum of squares\")\n",
    "# Show the legend.\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9203a7e",
   "metadata": {},
   "source": [
    "Let us remind ourselves of how the sum of squared error values change as we change the slope and the intercept.  First we hold the slope constant at a fairly bad guess of 0.1, and try different intercepts.  For each intercept we calculate the sum of squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31207991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some intercepts to try.\n",
    "intercepts = np.linspace(-2, 1, 1000)\n",
    "n_inters = len(intercepts)\n",
    "ss_errors = np.zeros(n_inters)\n",
    "for i in np.arange(n_inters):\n",
    "    ss_errors[i] = ss_cost([intercepts[i], 0.1], hemoglobin, appetite_d)\n",
    "plt.scatter(intercepts, ss_errors)\n",
    "plt.xlabel('intercept')\n",
    "plt.ylabel('Linear SS error')\n",
    "plt.title(\"Errors for different intercepts, slope 0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bd829b",
   "metadata": {},
   "source": [
    "Notice the very simple shape of this curve.  It is a parabola, it descends\n",
    "steeply for values far from the minimum, and more slowly as it gets closer.\n",
    "This is a curve that `minimize` finds it very easy to work with, because every\n",
    "time it tries an intercept (in this case), the direction (up, down) of the\n",
    "curve tells `minimize` what direction to go next.  If the curve is going down\n",
    "at this point, it should try a larger (more positive) intercept value; if the\n",
    "curve is going up, it should try a smaller (more negative) intercept. The\n",
    "up/down-ness of the curve tells `minimize` the right way to go, and this\n",
    "direction is always correct.  You may also have noticed that this parabola\n",
    "shape is always the same for these simple least squares functions, like\n",
    "`ss_any_line`.\n",
    "\n",
    "Just to illustrate again, here we try holding the intercept constant at a fairly bad guess of 0.5, and vary the slopes.  Notice we get the same helpful parabola shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8453147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slopes to try.\n",
    "slopes = np.linspace(-0.25, 0.25, 1000)\n",
    "n_slopes = len(slopes)\n",
    "ss_errors = np.zeros(n_slopes)\n",
    "for i in np.arange(n_slopes):\n",
    "    ss_errors[i] = ss_cost([0.5, slopes[i]], hemoglobin, appetite_d)\n",
    "plt.scatter(slopes, ss_errors)\n",
    "plt.xlabel('slope')\n",
    "plt.ylabel('Linear SS error')\n",
    "plt.title(\"Errors for different slopes, intercept 0.5\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f31170",
   "metadata": {},
   "source": [
    "These are plots of how the value of the *cost function* changes as we change\n",
    "the parameters.  The parabolas we see above are examples of curves that are\n",
    "[convex](https://en.wikipedia.org/wiki/Convex_function); convex curves like\n",
    "parabolas are particularly easy and quick for `minimize` to work with.\n",
    "\n",
    "We will see that using sum of squared error with our preferred sigmoid\n",
    "prediction generates cost function curves that are a lot more complicated,\n",
    "making it more difficult for `minimize` to find the best parameters.  If we\n",
    "give `minimize` a bad initial guess, it can get the answer badly wrong. Put\n",
    "technically, this is because the cost function curves are not convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32884b5",
   "metadata": {},
   "source": [
    "## Sigmoid prediction with sum of squares error\n",
    "\n",
    "For the reasons you saw in the [logistic regression page](logistic\n",
    "regression), we recoil from the very simple straight line fit above, and\n",
    "prefer to use a sigmoid curve to fit the 0 / 1 labels.\n",
    "\n",
    "In that page we defined the functions to convert the straight line predictions\n",
    "that we want to use with `minimize` and the sigmoid predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c79d3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_logit(y):\n",
    "    \"\"\" Reverse logit transformation\n",
    "    \"\"\"\n",
    "    odds_ratios = np.exp(y)  # Reverse the log operation.\n",
    "    return odds_ratios / (odds_ratios + 1)  # Reverse odds ratios operation.\n",
    "\n",
    "\n",
    "def params2pps(intercept, slope, x):\n",
    "    \"\"\" Calculate predicted probabilities of 1 for each observation.\n",
    "    \"\"\"\n",
    "    # Predicted log odds of being in class 1.\n",
    "    predicted_log_odds = intercept + slope * x\n",
    "    return inv_logit(predicted_log_odds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e8037a",
   "metadata": {},
   "source": [
    "This allowed us to build our sum of squares logit cost function.  This\n",
    "function calculates the sum of squares difference from the sigmoid predictions\n",
    "and the actual 0 / 1 labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa7caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ss_logit_cost(c_s, x_values, y_values):\n",
    "    # Unpack intercept and slope into values.\n",
    "    intercept, slope = c_s\n",
    "    # Predicted p values on sigmoid\n",
    "    pps = params2pps(intercept, slope, x_values)\n",
    "    # Prediction errors.\n",
    "    sigmoid_error = y_values - pps\n",
    "    # Sum of squared error\n",
    "    return np.sum(sigmoid_error ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2532c0b2",
   "metadata": {},
   "source": [
    "Then we found our sum of squares best straight line (that corresponds to a\n",
    "sigmoid after transformation).  Notice that we have started `minimize` with a\n",
    "guessed intercept of -7 and a guessed slope of 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcf1633",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_ss_logit = minimize(ss_logit_cost, [-7, 0.8], args=(hemoglobin, appetite_d))\n",
    "mr_ss_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb35018",
   "metadata": {},
   "source": [
    "We can calculate the predicted 0 / 1 labels, and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2325aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_ss_logit, slope_ss_logit = mr_ss_logit.x\n",
    "predicted_ss_logit = params2pps(inter_ss_logit, slope_ss_logit, hemoglobin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3645e8cb",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "plot_hgb_app()\n",
    "# A new plot on top of the old.\n",
    "plt.scatter(hemoglobin, predicted_ss_logit,\n",
    "            label='Logit ss solution',\n",
    "            color='gold')\n",
    "# Show the legend.\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28db17ce",
   "metadata": {},
   "source": [
    "Let us have a look at what the cost function curves look like for the `ss_logit_cost` cost function.  For now, let us look at what happens to the cost function curves as we change the intercept, holding the slope the same.\n",
    "\n",
    "Because we will do this several times, with various intercepts and constant slopes, we make a function, so we don't repeat ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054cdf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_some_intercepts(cost_function, intercepts, slope):\n",
    "    \"\"\" Plot values of `cost_function` for given `intercepts` and `slope`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cost_function : function\n",
    "        Function to call to get cost function value, given an intercept and a slope.\n",
    "    intercepts : array\n",
    "        Array of intercepts for which to calculate cost function.\n",
    "    slope : number\n",
    "        Slope (held constant for each intercept).\n",
    "    \"\"\"\n",
    "    n = len(intercepts)\n",
    "    ss_errors = np.zeros(n)\n",
    "    for i in np.arange(n):\n",
    "        # Calculate and store cost function value for intercept, slope.\n",
    "        ss_errors[i] = cost_function([intercepts[i], slope],\n",
    "                                     hemoglobin, appetite_d)\n",
    "    plt.plot(intercepts, ss_errors, linewidth=1)\n",
    "    plt.xlabel('intercept')\n",
    "    plt.ylabel('Cost function value');\n",
    "    plt.title('Errors for slope = %.2f' % slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad480e4",
   "metadata": {},
   "source": [
    "Remember the attractive parabolas for the cost function curves in the crude\n",
    "case above, where we were doing simple regression.\n",
    "\n",
    "In the next cell, we set the slope to the best slope that `minimize`\n",
    "found, and show the effect on our `ss_logit_cost` function, when varying the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a2863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function curve varying intercept between -20 and 5, for best slope.\n",
    "plot_some_intercepts(ss_logit_cost, np.linspace(-20, 5, 1000), slope_ss_logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3102b1",
   "metadata": {},
   "source": [
    "There is a clear minimum at around -5.2, as we expect from the results above,\n",
    "but we have lost the nice parabola shape.  For intercepts greater than about\n",
    "3, the graph is very flat.  This could spell trouble for `minimize`, if it\n",
    "gets stuck trying a series of intercepts more than 3.  For example, the cost\n",
    "function will stay almost the same as `minimize` tries values around 5, so\n",
    "`minimize` may not discover that it needs to track back to the real minimum,\n",
    "near -5.\n",
    "\n",
    "It can get even worse when trying slopes that are further away from the\n",
    "optimum.  In the next plot, we set the slope badly wrong, at 3, and try different intercepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c754cdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_some_intercepts(ss_logit_cost, np.linspace(-30, 25, 1000), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c99917",
   "metadata": {},
   "source": [
    "The plot is a strange shape. Again we see a nasty plateau with intercepts\n",
    "above 0.  If `minimize` is trying intercepts above 0, the cost function may\n",
    "not vary much, and `minimize` may get stuck on this plateau, for example\n",
    "concluding the intercept of 6 is as good as any nearby.\n",
    "\n",
    "An in fact, this does happen if we set very bad starting estimates for\n",
    "`minimize`.  Here we set the starting intercept to be 6, and the starting\n",
    "slope to be 2.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdbc89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_mr_ss_logit = minimize(ss_logit_cost, [6, 2.5],\n",
    "                           args=(hemoglobin, appetite_d))\n",
    "bad_mr_ss_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f543e8e",
   "metadata": {},
   "source": [
    "You can see that `minimize` has got stuck on the plateau we saw above and has\n",
    "given up, simply returning the terrible intercept and slope we sent it.\n",
    "\n",
    "You can also see that `minimize` did not detect any problems, and returned the\n",
    "message \"Optimization terminated successfully\".\n",
    "\n",
    "We have this problem because of the irregular shape of the cost-function curve for our cost function, that calculates sum of squared error for the sigmoid predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecf7ff2",
   "metadata": {},
   "source": [
    "## Back to maximum likelihood\n",
    "\n",
    "The [logistic_regression](logistic_regression) page proposed an alternative\n",
    "cost function for the sigmoid predictions - maximum likelihood.  See that page\n",
    "for details, but here is somewhat cleaned up version the maximum likelihood\n",
    "cost function from the page above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ea385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_logit_cost(intercept_and_slope, x, y):\n",
    "    \"\"\" Cost function for maximum likelihood\n",
    "    \"\"\"\n",
    "    intercept, slope = intercept_and_slope\n",
    "    pp1 = params2pps(intercept, slope, x)\n",
    "    p_of_y = y * pp1 + (1 - y) * (1 - pp1)\n",
    "    log_likelihood = np.sum(np.log(p_of_y))\n",
    "    return -log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8eb26e",
   "metadata": {},
   "source": [
    "We find the best intercept and slope using the maximum likelihood (ML).  While\n",
    "we are at it, we send in the same terrible estimate for the intercept and\n",
    "slope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2882b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_ML = minimize(ml_logit_cost, [6, 2.5], args=(hemoglobin, appetite_d))\n",
    "mr_ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955ed342",
   "metadata": {},
   "source": [
    "The fit is reasonable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a58ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_ML, slope_ML = mr_ML.x\n",
    "predicted_ML = inv_logit(inter_ML + slope_ML * hemoglobin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b86a497",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "plot_hgb_app()\n",
    "plt.scatter(hemoglobin, predicted_ML, c='gold', label='ML prediction')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d01293",
   "metadata": {},
   "source": [
    "The ML search by `minimize` is more reliable than the sum-of-squares case\n",
    "above; it is less dependent on us choosing some reasonable starting values.\n",
    "This is because the ML cost function is *convex*. Here is the cost-function\n",
    "curve for the ML cost function, as we vary the intercept for a fixed slope. We\n",
    "see that we have a much more predictable curve, that slopes smoothly downwards\n",
    "to a minimum and smoothly upwards after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8d5dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_some_intercepts(ml_logit_cost, np.linspace(-30, 25, 1000), slope_ss_logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85583446",
   "metadata": {},
   "source": [
    "However, this does not mean the ML cost function is infallible.  We can push it to a state where the calculation errors start to overwhelm the values.   However, ML still has the advantage, because, unlike the sum of squares, we do get a warning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96858eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting estimates too far off.\n",
    "minimize(ml_logit_cost, [6, 3], args=(hemoglobin, appetite_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1ab5dc",
   "metadata": {},
   "source": [
    "## Another example\n",
    "\n",
    "You will find another demonstration of this difference between sum of squares\n",
    "and maximum likelihood in [this on logistic cost functions](logistic_flails)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "all,-language_info",
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
